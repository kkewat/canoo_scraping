# -*- coding: utf-8 -*-
"""scrape_canoo_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bdJXkjUj-Fxe1HiL9UPX2SiuI_zUnUXb
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

# List of URLs to scrape
urls = [
    'https://en.wikipedia.org/wiki/Canoo',
    'https://www.usatoday.com/story/money/cars/2023/11/22/a-2024-canoo-lifestyle-vehicle-first-drive-review/71672558007/',
    'https://www.theverge.com/2023/7/12/23792450/canoo-ev-nasa-artemis-defense-government',
    'https://auto.economictimes.indiatimes.com/tag/canoo',
    'https://www.3ds.com/insights/customer-stories/canoo',
    'https://www.autoweek.com/news/a45892887/canoo-american-bulldog-truck/'
]

all_reviews = []

for url in urls:
    print(f"Scraping reviews from {url}")
    response = requests.get(url)
    content = response.content
    parsed_content = BeautifulSoup(content, 'html.parser')

    # Extracting text content from different elements depending on the structure of the page
    if 'wikipedia' in url:
        reviews = [para.text for para in parsed_content.find_all("p")]
    elif 'usatoday' in url:
        reviews = [parsed_content.find("div", class_="gnt_ar_b").text]
    elif 'theverge' in url:
        reviews = [parsed_content.find("article").text]
    elif 'economictimes' in url:
        review_element = parsed_content.find("div", class_="content")
        reviews = [review_element.text] if review_element else []
    elif '3ds.com' in url:
        review_element = parsed_content.find("div", class_="main-text")
        reviews = [review_element.text] if review_element else []
    elif 'autoweek' in url:
        review_element = parsed_content.find("div", class_="main-text")
        reviews = [review_element.text] if review_element else []
    else:
        reviews = []

    all_reviews.extend(reviews)

df = pd.DataFrame({'reviews': all_reviews})
df.to_csv("canoo_reviews.csv", index=False)

!pip install summa

import pandas as pd
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from summa import summarizer
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

#Text Preprocessing
stop_words = set(stopwords.words("english"))
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    tokens = word_tokenize(text.lower())  # Convert to lowercase and tokenize
    tokens = [token for token in tokens if token.isalnum()]  # Remove punctuation
    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]  # Lemmatize and remove stopwords
    return " ".join(tokens)

df["cleaned_reviews"] = df["reviews"].apply(preprocess_text)

#Vectorization (TF-IDF)
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(df["cleaned_reviews"])

#Text Summarization
def generate_summary(text):
    try:
        return summarizer.summarize(text)
    except ValueError:  # If text is too short for summarization
        return text

df["summary"] = df["reviews"].apply(generate_summary)

#Report Generation
report_df = df[["reviews", "summary"]]
report_df.to_csv("canoo_reviews_summary.csv", index=False)

# Display the report DataFrame
print(report_df)